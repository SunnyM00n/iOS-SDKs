// swift-interface-format-version: 1.0
// swift-compiler-version: Apple Swift version 6.2 effective-5.10 (swiftlang-6.2.0.17.14 clang-1700.3.17.1)
// swift-module-flags: -target arm64e-apple-ios26.0 -enable-objc-interop -enable-library-evolution -swift-version 5 -enforce-exclusivity=checked -Osize -library-level api -enable-upcoming-feature InternalImportsByDefault -enable-upcoming-feature MemberImportVisibility -enable-experimental-feature DebugDescriptionMacro -enable-bare-slash-regex -user-module-version 3500.107.3.11.1 -module-name Speech -package-name com.apple.Speech
// swift-module-flags-ignorable:  -formal-cxx-interoperability-mode=off -interface-compiler-version 6.2
@_exported public import Speech
public import Swift
public import _Concurrency
public import _StringProcessing
public import _SwiftConcurrencyShims
@_hasMissingDesignatedInitializers @available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
final public class AssetInventory {
  public static var maximumReservedLocales: Swift.Int {
    get
  }
  public static var reservedLocales: [Foundation.Locale] {
    get async
  }
  @discardableResult
  public static func reserve(locale: Foundation.Locale) async throws -> Swift.Bool
  @discardableResult
  public static func release(reservedLocale: Foundation.Locale) async -> Swift.Bool
  public enum Status : Swift.Comparable {
    case unsupported
    case supported
    case downloading
    case installed
    public static func < (a: Speech.AssetInventory.Status, b: Speech.AssetInventory.Status) -> Swift.Bool
    public static func == (a: Speech.AssetInventory.Status, b: Speech.AssetInventory.Status) -> Swift.Bool
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  public static func status(forModules modules: [any Speech.SpeechModule]) async -> Speech.AssetInventory.Status
  #if compiler(>=5.3) && $NonescapableTypes
  public static func assetInstallationRequest(supporting modules: [any Speech.SpeechModule]) async throws -> Speech.AssetInstallationRequest?
  #endif
  @objc deinit
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
public protocol LocaleDependentSpeechModule : Speech.SpeechModule {
  static var supportedLocales: [Foundation.Locale] { get async }
  #if compiler(>=5.3) && $NonescapableTypes
  static func supportedLocale(equivalentTo locale: Foundation.Locale) async -> Foundation.Locale?
  #endif
  var selectedLocales: [Foundation.Locale] { get }
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
final public class DictationTranscriber : Speech.LocaleDependentSpeechModule {
  convenience public init(locale: Foundation.Locale, preset: Speech.DictationTranscriber.Preset)
  convenience public init(locale: Foundation.Locale, contentHints: Swift.Set<Speech.DictationTranscriber.ContentHint>, transcriptionOptions: Swift.Set<Speech.DictationTranscriber.TranscriptionOption>, reportingOptions: Swift.Set<Speech.DictationTranscriber.ReportingOption>, attributeOptions: Swift.Set<Speech.DictationTranscriber.ResultAttributeOption>)
  public struct Preset : Swift.Sendable, Swift.Equatable, Swift.Hashable {
    public static let phrase: Speech.DictationTranscriber.Preset
    public static let shortDictation: Speech.DictationTranscriber.Preset
    public static let progressiveShortDictation: Speech.DictationTranscriber.Preset
    public static let longDictation: Speech.DictationTranscriber.Preset
    public static let progressiveLongDictation: Speech.DictationTranscriber.Preset
    public static let timeIndexedLongDictation: Speech.DictationTranscriber.Preset
    public init(contentHints: Swift.Set<Speech.DictationTranscriber.ContentHint>, transcriptionOptions: Swift.Set<Speech.DictationTranscriber.TranscriptionOption>, reportingOptions: Swift.Set<Speech.DictationTranscriber.ReportingOption>, attributeOptions: Swift.Set<Speech.DictationTranscriber.ResultAttributeOption>)
    public var contentHints: Swift.Set<Speech.DictationTranscriber.ContentHint>
    public var transcriptionOptions: Swift.Set<Speech.DictationTranscriber.TranscriptionOption>
    public var reportingOptions: Swift.Set<Speech.DictationTranscriber.ReportingOption>
    public var attributeOptions: Swift.Set<Speech.DictationTranscriber.ResultAttributeOption>
    public static func == (a: Speech.DictationTranscriber.Preset, b: Speech.DictationTranscriber.Preset) -> Swift.Bool
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  public struct ContentHint : Swift.Sendable, Swift.Equatable, Swift.Hashable {
    public static let shortForm: Speech.DictationTranscriber.ContentHint
    public static let farField: Speech.DictationTranscriber.ContentHint
    public static let atypicalSpeech: Speech.DictationTranscriber.ContentHint
    public static func customizedLanguage(modelConfiguration: Speech.SFSpeechLanguageModel.Configuration) -> Speech.DictationTranscriber.ContentHint
    public static func == (a: Speech.DictationTranscriber.ContentHint, b: Speech.DictationTranscriber.ContentHint) -> Swift.Bool
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  public enum TranscriptionOption : Swift.CaseIterable, Swift.Sendable, Swift.Equatable, Swift.Hashable {
    case punctuation
    case emoji
    case etiquetteReplacements
    public static func == (a: Speech.DictationTranscriber.TranscriptionOption, b: Speech.DictationTranscriber.TranscriptionOption) -> Swift.Bool
    @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
    @available(tvOS, unavailable)
    @available(watchOS, unavailable)
    public typealias AllCases = [Speech.DictationTranscriber.TranscriptionOption]
    nonisolated public static var allCases: [Speech.DictationTranscriber.TranscriptionOption] {
      get
    }
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  public enum ReportingOption : Swift.CaseIterable, Swift.Sendable, Swift.Equatable, Swift.Hashable {
    case volatileResults
    case alternativeTranscriptions
    case frequentFinalization
    public static func == (a: Speech.DictationTranscriber.ReportingOption, b: Speech.DictationTranscriber.ReportingOption) -> Swift.Bool
    @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
    @available(tvOS, unavailable)
    @available(watchOS, unavailable)
    public typealias AllCases = [Speech.DictationTranscriber.ReportingOption]
    nonisolated public static var allCases: [Speech.DictationTranscriber.ReportingOption] {
      get
    }
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  public enum ResultAttributeOption : Swift.CaseIterable, Swift.Sendable, Swift.Equatable, Swift.Hashable {
    case audioTimeRange
    case transcriptionConfidence
    public static func == (a: Speech.DictationTranscriber.ResultAttributeOption, b: Speech.DictationTranscriber.ResultAttributeOption) -> Swift.Bool
    @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
    @available(tvOS, unavailable)
    @available(watchOS, unavailable)
    public typealias AllCases = [Speech.DictationTranscriber.ResultAttributeOption]
    nonisolated public static var allCases: [Speech.DictationTranscriber.ResultAttributeOption] {
      get
    }
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  public static var supportedLocales: [Foundation.Locale] {
    get async
  }
  #if compiler(>=5.3) && $NonescapableTypes
  public static func supportedLocale(equivalentTo locale: Foundation.Locale) async -> Foundation.Locale?
  #endif
  public static var installedLocales: [Foundation.Locale] {
    get async
  }
  final public var selectedLocales: [Foundation.Locale] {
    get
  }
  final public var availableCompatibleAudioFormats: [AVFAudio.AVAudioFormat] {
    get async
  }
  final public var results: some Swift.Sendable & _Concurrency.AsyncSequence<Speech.DictationTranscriber.Result, any Swift.Error> {
    get
  }
  public struct Result : Speech.SpeechModuleResult, Swift.Sendable, Swift.CustomStringConvertible, Swift.Equatable, Swift.Hashable {
    public let range: CoreMedia.CMTimeRange
    public let resultsFinalizationTime: CoreMedia.CMTime
    public var text: Foundation.AttributedString {
      get
    }
    public let alternatives: [Foundation.AttributedString]
    public var description: Swift.String {
      get
    }
    public static func == (a: Speech.DictationTranscriber.Result, b: Speech.DictationTranscriber.Result) -> Swift.Bool
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
  @available(tvOS, unavailable)
  @available(watchOS, unavailable)
  public typealias Results = @_opaqueReturnTypeOf("$s6Speech20DictationTranscriberC7resultsQrvp", 0) __
  @objc deinit
}
extension Foundation.AttributeScopes {
  @available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
  @available(tvOS, unavailable)
  @available(watchOS, unavailable)
  public struct SpeechAttributes : Foundation.AttributeScope {
    public let transcriptionConfidence: Foundation.AttributeScopes.SpeechAttributes.ConfidenceAttribute
    public let audioTimeRange: Foundation.AttributeScopes.SpeechAttributes.TimeRangeAttribute
    public struct ConfidenceAttribute : Foundation.CodableAttributedStringKey {
      public static let name: Swift.String
      public typealias Value = Swift.Double
    }
    public struct TimeRangeAttribute : Foundation.CodableAttributedStringKey {
      public static let name: Swift.String
      public typealias Value = CoreMedia.CMTimeRange
      public static func encode(_ value: Foundation.AttributeScopes.SpeechAttributes.TimeRangeAttribute.Value, to encoder: any Swift.Encoder) throws
      public static func decode(from decoder: any Swift.Decoder) throws -> Foundation.AttributeScopes.SpeechAttributes.TimeRangeAttribute.Value
    }
    @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
    @available(tvOS, unavailable)
    @available(watchOS, unavailable)
    public typealias DecodingConfiguration = Foundation.AttributeScopeCodableConfiguration
    @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
    @available(tvOS, unavailable)
    @available(watchOS, unavailable)
    public typealias EncodingConfiguration = Foundation.AttributeScopeCodableConfiguration
  }
}
extension Foundation.AttributeDynamicLookup {
  @available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
  @available(tvOS, unavailable)
  @available(watchOS, unavailable)
  public subscript<T>(dynamicMember keyPath: Swift.KeyPath<Foundation.AttributeScopes.SpeechAttributes, T>) -> T where T : Foundation.AttributedStringKey {
    get
  }
}
extension Foundation.AttributedString {
  #if compiler(>=5.3) && $NonescapableTypes
  @available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
  @available(tvOS, unavailable)
  @available(watchOS, unavailable)
  public func rangeOfAudioTimeRangeAttributes(intersecting timeRange: CoreMedia.CMTimeRange) -> Swift.Range<Foundation.AttributedString.Index>?
  #endif
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
final public actor SpeechAnalyzer : Swift.Sendable {
  #if compiler(>=5.3) && $NonescapableTypes
  public convenience init(modules: [any Speech.SpeechModule], options: Speech.SpeechAnalyzer.Options? = nil)
  #endif
  #if compiler(>=5.3) && $NonescapableTypes
  #if compiler(>=5.3) && $SendingArgsAndResults
  public convenience init<InputSequence>(inputSequence: InputSequence, modules: [any Speech.SpeechModule], options: Speech.SpeechAnalyzer.Options? = nil, analysisContext: Speech.AnalysisContext = .init(), volatileRangeChangedHandler: sending ((_ range: CoreMedia.CMTimeRange, _ changedStart: Swift.Bool, _ changedEnd: Swift.Bool) -> Swift.Void)? = nil) where InputSequence : Swift.Sendable, InputSequence : _Concurrency.AsyncSequence, InputSequence.Element == Speech.AnalyzerInput
  #else
  public convenience init<InputSequence>(inputSequence: InputSequence, modules: [any Speech.SpeechModule], options: Speech.SpeechAnalyzer.Options? = nil, analysisContext: Speech.AnalysisContext = .init(), volatileRangeChangedHandler: __owned ((_ range: CoreMedia.CMTimeRange, _ changedStart: Swift.Bool, _ changedEnd: Swift.Bool) -> Swift.Void)? = nil) where InputSequence : Swift.Sendable, InputSequence : _Concurrency.AsyncSequence, InputSequence.Element == Speech.AnalyzerInput
  #endif
  #endif
  #if compiler(>=5.3) && $NonescapableTypes
  final public func prepareToAnalyze(in audioFormat: AVFAudio.AVAudioFormat?) async throws
  #endif
  #if compiler(>=5.3) && $NonescapableTypes
  #if compiler(>=5.3) && $SendingArgsAndResults
  final public func prepareToAnalyze(in audioFormat: AVFAudio.AVAudioFormat?, withProgressReadyHandler progressReadyHandler: sending ((Foundation.Progress) -> Swift.Void)?) async throws
  #else
  final public func prepareToAnalyze(in audioFormat: AVFAudio.AVAudioFormat?, withProgressReadyHandler progressReadyHandler: __owned ((Foundation.Progress) -> Swift.Void)?) async throws
  #endif
  #endif
  @objc deinit
  final public var modules: [any Speech.SpeechModule] {
    get
  }
  final public func setModules(_ newModules: [any Speech.SpeechModule]) async throws
  final public func start<InputSequence>(inputSequence: InputSequence) async throws where InputSequence : Swift.Sendable, InputSequence : _Concurrency.AsyncSequence, InputSequence.Element == Speech.AnalyzerInput
  #if compiler(>=5.3) && $NonescapableTypes
  final public func analyzeSequence<InputSequence>(_ inputSequence: InputSequence) async throws -> CoreMedia.CMTime? where InputSequence : Swift.Sendable, InputSequence : _Concurrency.AsyncSequence, InputSequence.Element == Speech.AnalyzerInput
  #endif
  #if compiler(>=5.3) && $NonescapableTypes
  final public func finalize(through: CoreMedia.CMTime?) async throws
  #endif
  final public func finalizeAndFinishThroughEndOfInput() async throws
  final public func finalizeAndFinish(through: CoreMedia.CMTime) async throws
  final public func finish(after: CoreMedia.CMTime) async throws
  final public func cancelAnalysis(before: CoreMedia.CMTime)
  final public func cancelAndFinishNow() async
  #if compiler(>=5.3) && $NonescapableTypes
  final public var volatileRange: CoreMedia.CMTimeRange? {
    get
  }
  #endif
  #if compiler(>=5.3) && $NonescapableTypes
  #if compiler(>=5.3) && $SendingArgsAndResults
  final public func setVolatileRangeChangedHandler(_ handler: sending ((_ range: CoreMedia.CMTimeRange, _ changedStart: Swift.Bool, _ changedEnd: Swift.Bool) -> Swift.Void)?)
  #else
  final public func setVolatileRangeChangedHandler(_ handler: __owned ((_ range: CoreMedia.CMTimeRange, _ changedStart: Swift.Bool, _ changedEnd: Swift.Bool) -> Swift.Void)?)
  #endif
  #endif
  final public var context: Speech.AnalysisContext {
    get async
  }
  final public func setContext(_ newContext: Speech.AnalysisContext) async throws
  #if compiler(>=5.3) && $NonescapableTypes
  public static func bestAvailableAudioFormat(compatibleWith modules: [any Speech.SpeechModule]) async -> AVFAudio.AVAudioFormat?
  #endif
  #if compiler(>=5.3) && $NonescapableTypes
  public static func bestAvailableAudioFormat(compatibleWith modules: [any Speech.SpeechModule], considering naturalFormat: AVFAudio.AVAudioFormat?) async -> AVFAudio.AVAudioFormat?
  #endif
  @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
  @available(tvOS, unavailable)
  @available(watchOS, unavailable)
  @_semantics("defaultActor") nonisolated final public var unownedExecutor: _Concurrency.UnownedSerialExecutor {
    get
  }
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
public struct AnalyzerInput : @unchecked Swift.Sendable {
  public init(buffer: AVFAudio.AVAudioPCMBuffer)
  #if compiler(>=5.3) && $NonescapableTypes
  public init(buffer: AVFAudio.AVAudioPCMBuffer, bufferStartTime: CoreMedia.CMTime?)
  #endif
  public let buffer: AVFAudio.AVAudioPCMBuffer
  public let bufferStartTime: CoreMedia.CMTime?
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
public enum SpeechModels {
  public static func endRetention() async
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
final public class SpeechDetector {
  public init(detectionOptions: Speech.SpeechDetector.DetectionOptions, reportResults: Swift.Bool)
  convenience public init()
  public enum SensitivityLevel : Swift.Int, Swift.CaseIterable, Swift.Sendable, Swift.Equatable, Swift.Hashable {
    case low
    case medium
    case high
    #if compiler(>=5.3) && $NonescapableTypes
    public init?(rawValue: Swift.Int)
    #endif
    @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
    @available(tvOS, unavailable)
    @available(watchOS, unavailable)
    public typealias AllCases = [Speech.SpeechDetector.SensitivityLevel]
    @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
    @available(tvOS, unavailable)
    @available(watchOS, unavailable)
    public typealias RawValue = Swift.Int
    nonisolated public static var allCases: [Speech.SpeechDetector.SensitivityLevel] {
      get
    }
    public var rawValue: Swift.Int {
      get
    }
  }
  public struct DetectionOptions : Swift.Sendable, Swift.Equatable, Swift.Hashable {
    public let sensitivityLevel: Speech.SpeechDetector.SensitivityLevel
    public init(sensitivityLevel: Speech.SpeechDetector.SensitivityLevel)
    public static func == (a: Speech.SpeechDetector.DetectionOptions, b: Speech.SpeechDetector.DetectionOptions) -> Swift.Bool
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  final public var results: some Swift.Sendable & _Concurrency.AsyncSequence<Speech.SpeechDetector.Result, any Swift.Error> {
    get
  }
  public struct Result : Speech.SpeechModuleResult, Swift.Sendable, Swift.CustomStringConvertible {
    public let range: CoreMedia.CMTimeRange
    public let resultsFinalizationTime: CoreMedia.CMTime
    public let speechDetected: Swift.Bool
    public var description: Swift.String {
      get
    }
  }
  final public var availableCompatibleAudioFormats: [AVFAudio.AVAudioFormat] {
    get
  }
  @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
  @available(tvOS, unavailable)
  @available(watchOS, unavailable)
  public typealias Results = @_opaqueReturnTypeOf("$s6Speech0A8DetectorC7resultsQrvp", 0) __
  @objc deinit
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
public protocol SpeechModule : AnyObject, Swift.Sendable {
  var results: Self.Results { get }
  associatedtype Results : Swift.Sendable, _Concurrency.AsyncSequence where Self.Results.Failure == any Swift.Error
  associatedtype Result : Speech.SpeechModuleResult, Swift.Sendable where Self.Result == Self.Results.Element
  var availableCompatibleAudioFormats: [AVFAudio.AVAudioFormat] { get async }
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
public protocol SpeechModuleResult {
  var range: CoreMedia.CMTimeRange { get }
  var resultsFinalizationTime: CoreMedia.CMTime { get }
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
extension Speech.SpeechModuleResult {
  public var isFinal: Swift.Bool {
    get
  }
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
extension Speech.SpeechAnalyzer {
  #if compiler(>=5.3) && $NonescapableTypes
  #if compiler(>=5.3) && $SendingArgsAndResults
  public convenience init(inputAudioFile: AVFAudio.AVAudioFile, modules: [any Speech.SpeechModule], options: Speech.SpeechAnalyzer.Options? = nil, analysisContext: Speech.AnalysisContext = .init(), finishAfterFile: Swift.Bool = false, volatileRangeChangedHandler: sending ((_ range: CoreMedia.CMTimeRange, _ changedStart: Swift.Bool, _ changedEnd: Swift.Bool) -> Swift.Void)? = nil) async throws
  #else
  public convenience init(inputAudioFile: AVFAudio.AVAudioFile, modules: [any Speech.SpeechModule], options: Speech.SpeechAnalyzer.Options? = nil, analysisContext: Speech.AnalysisContext = .init(), finishAfterFile: Swift.Bool = false, volatileRangeChangedHandler: __owned ((_ range: CoreMedia.CMTimeRange, _ changedStart: Swift.Bool, _ changedEnd: Swift.Bool) -> Swift.Void)? = nil) async throws
  #endif
  #endif
  final public func start(inputAudioFile audioFile: AVFAudio.AVAudioFile, finishAfterFile: Swift.Bool = false) async throws
  #if compiler(>=5.3) && $NonescapableTypes
  final public func analyzeSequence(from audioFile: AVFAudio.AVAudioFile) async throws -> CoreMedia.CMTime?
  #endif
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
final public class SpeechTranscriber : Speech.LocaleDependentSpeechModule {
  convenience public init(locale: Foundation.Locale, preset: Speech.SpeechTranscriber.Preset)
  convenience public init(locale: Foundation.Locale, transcriptionOptions: Swift.Set<Speech.SpeechTranscriber.TranscriptionOption>, reportingOptions: Swift.Set<Speech.SpeechTranscriber.ReportingOption>, attributeOptions: Swift.Set<Speech.SpeechTranscriber.ResultAttributeOption>)
  public struct Preset : Swift.Sendable, Swift.Equatable, Swift.Hashable {
    public static let transcription: Speech.SpeechTranscriber.Preset
    public static let transcriptionWithAlternatives: Speech.SpeechTranscriber.Preset
    public static let timeIndexedTranscriptionWithAlternatives: Speech.SpeechTranscriber.Preset
    public static let progressiveTranscription: Speech.SpeechTranscriber.Preset
    public static let timeIndexedProgressiveTranscription: Speech.SpeechTranscriber.Preset
    public init(transcriptionOptions: Swift.Set<Speech.SpeechTranscriber.TranscriptionOption>, reportingOptions: Swift.Set<Speech.SpeechTranscriber.ReportingOption>, attributeOptions: Swift.Set<Speech.SpeechTranscriber.ResultAttributeOption>)
    public var transcriptionOptions: Swift.Set<Speech.SpeechTranscriber.TranscriptionOption>
    public var reportingOptions: Swift.Set<Speech.SpeechTranscriber.ReportingOption>
    public var attributeOptions: Swift.Set<Speech.SpeechTranscriber.ResultAttributeOption>
    public static func == (a: Speech.SpeechTranscriber.Preset, b: Speech.SpeechTranscriber.Preset) -> Swift.Bool
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  public enum TranscriptionOption : Swift.CaseIterable, Swift.Sendable, Swift.Equatable, Swift.Hashable {
    case etiquetteReplacements
    public static func == (a: Speech.SpeechTranscriber.TranscriptionOption, b: Speech.SpeechTranscriber.TranscriptionOption) -> Swift.Bool
    @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
    @available(tvOS, unavailable)
    @available(watchOS, unavailable)
    public typealias AllCases = [Speech.SpeechTranscriber.TranscriptionOption]
    nonisolated public static var allCases: [Speech.SpeechTranscriber.TranscriptionOption] {
      get
    }
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  public enum ReportingOption : Swift.CaseIterable, Swift.Sendable, Swift.Equatable, Swift.Hashable {
    case volatileResults
    case alternativeTranscriptions
    case fastResults
    public static func == (a: Speech.SpeechTranscriber.ReportingOption, b: Speech.SpeechTranscriber.ReportingOption) -> Swift.Bool
    @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
    @available(tvOS, unavailable)
    @available(watchOS, unavailable)
    public typealias AllCases = [Speech.SpeechTranscriber.ReportingOption]
    nonisolated public static var allCases: [Speech.SpeechTranscriber.ReportingOption] {
      get
    }
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  public enum ResultAttributeOption : Swift.CaseIterable, Swift.Sendable, Swift.Equatable, Swift.Hashable {
    case audioTimeRange
    case transcriptionConfidence
    public static func == (a: Speech.SpeechTranscriber.ResultAttributeOption, b: Speech.SpeechTranscriber.ResultAttributeOption) -> Swift.Bool
    @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
    @available(tvOS, unavailable)
    @available(watchOS, unavailable)
    public typealias AllCases = [Speech.SpeechTranscriber.ResultAttributeOption]
    nonisolated public static var allCases: [Speech.SpeechTranscriber.ResultAttributeOption] {
      get
    }
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  public static var isAvailable: Swift.Bool {
    get
  }
  public static var supportedLocales: [Foundation.Locale] {
    get async
  }
  #if compiler(>=5.3) && $NonescapableTypes
  public static func supportedLocale(equivalentTo locale: Foundation.Locale) async -> Foundation.Locale?
  #endif
  public static var installedLocales: [Foundation.Locale] {
    get async
  }
  final public var selectedLocales: [Foundation.Locale] {
    get
  }
  final public var availableCompatibleAudioFormats: [AVFAudio.AVAudioFormat] {
    get async
  }
  final public var results: some Swift.Sendable & _Concurrency.AsyncSequence<Speech.SpeechTranscriber.Result, any Swift.Error> {
    get
  }
  public struct Result : Speech.SpeechModuleResult, Swift.Sendable, Swift.CustomStringConvertible, Swift.Equatable, Swift.Hashable {
    public let range: CoreMedia.CMTimeRange
    public let resultsFinalizationTime: CoreMedia.CMTime
    public var text: Foundation.AttributedString {
      get
    }
    public let alternatives: [Foundation.AttributedString]
    public var description: Swift.String {
      get
    }
    public static func == (a: Speech.SpeechTranscriber.Result, b: Speech.SpeechTranscriber.Result) -> Swift.Bool
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
  }
  @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
  @available(tvOS, unavailable)
  @available(watchOS, unavailable)
  public typealias Results = @_opaqueReturnTypeOf("$s6Speech0A11TranscriberC7resultsQrvp", 0) __
  @objc deinit
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
extension Speech.SpeechAnalyzer {
  public struct Options : Swift.Sendable, Swift.Equatable {
    public let priority: _Concurrency.TaskPriority
    public let modelRetention: Speech.SpeechAnalyzer.Options.ModelRetention
    public enum ModelRetention : Swift.CaseIterable, Swift.Sendable, Swift.Equatable, Swift.Hashable {
      case whileInUse
      case lingering
      case processLifetime
      public static func == (a: Speech.SpeechAnalyzer.Options.ModelRetention, b: Speech.SpeechAnalyzer.Options.ModelRetention) -> Swift.Bool
      @available(iOS 26.0, visionOS 26.0, macOS 26.0, *)
      @available(tvOS, unavailable)
      @available(watchOS, unavailable)
      public typealias AllCases = [Speech.SpeechAnalyzer.Options.ModelRetention]
      nonisolated public static var allCases: [Speech.SpeechAnalyzer.Options.ModelRetention] {
        get
      }
      public func hash(into hasher: inout Swift.Hasher)
      public var hashValue: Swift.Int {
        get
      }
    }
    public init(priority: _Concurrency.TaskPriority, modelRetention: Speech.SpeechAnalyzer.Options.ModelRetention)
    public static func == (a: Speech.SpeechAnalyzer.Options, b: Speech.SpeechAnalyzer.Options) -> Swift.Bool
  }
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
final public class AnalysisContext : Swift.Sendable {
  @objc public init()
  final public var contextualStrings: [Speech.AnalysisContext.ContextualStringsTag : [Swift.String]] {
    get
    set
  }
  final public var userData: [Speech.AnalysisContext.UserDataTag : any Swift.Sendable] {
    get
    set
  }
  public struct ContextualStringsTag : Swift.RawRepresentable, Swift.Sendable, Swift.Equatable, Swift.Hashable {
    public typealias RawValue = Swift.String
    public init(_ rawValue: Speech.AnalysisContext.ContextualStringsTag.RawValue)
    public init(rawValue: Speech.AnalysisContext.ContextualStringsTag.RawValue)
    public let rawValue: Speech.AnalysisContext.ContextualStringsTag.RawValue
    public static let general: Speech.AnalysisContext.ContextualStringsTag
  }
  public struct UserDataTag : Swift.RawRepresentable, Swift.Sendable, Swift.Equatable, Swift.Hashable {
    public typealias RawValue = Swift.String
    public init(_ rawValue: Speech.AnalysisContext.UserDataTag.RawValue)
    public init(rawValue: Speech.AnalysisContext.UserDataTag.RawValue)
    public let rawValue: Speech.AnalysisContext.UserDataTag.RawValue
  }
  @objc deinit
}
@objc @_hasMissingDesignatedInitializers @available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
final public class AssetInstallationRequest : ObjectiveC.NSObject, Foundation.ProgressReporting, Swift.Sendable {
  @objc final public var progress: Foundation.Progress {
    @objc get
  }
  final public func downloadAndInstall() async throws
  @objc deinit
}
@available(macOS 14, iOS 17, visionOS 1.1, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
public protocol DataInsertable {
  func insert(data: Speech.SFCustomLanguageModelData)
}
@available(macOS 14, iOS 17, visionOS 1.1, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
public protocol TemplateInsertable {
  func insert(generator: Speech.SFCustomLanguageModelData.TemplatePhraseCountGenerator)
}
@available(macOS 14, iOS 17, visionOS 1.1, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
public class SFCustomLanguageModelData : Swift.Hashable, Swift.Codable {
  public struct PhraseCount : Swift.Hashable, Swift.Sendable, Swift.CustomStringConvertible, Swift.Codable, Speech.DataInsertable {
    public let phrase: Swift.String
    public let count: Swift.Int
    public init(phrase: Swift.String, count: Swift.Int)
    public var description: Swift.String {
      get
    }
    public func insert(data: Speech.SFCustomLanguageModelData)
    public static func == (a: Speech.SFCustomLanguageModelData.PhraseCount, b: Speech.SFCustomLanguageModelData.PhraseCount) -> Swift.Bool
    public func encode(to encoder: any Swift.Encoder) throws
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
    public init(from decoder: any Swift.Decoder) throws
  }
  public struct CustomPronunciation : Swift.Hashable, Swift.Sendable, Swift.CustomStringConvertible, Swift.Codable, Speech.DataInsertable {
    public let grapheme: Swift.String
    public let phonemes: [Swift.String]
    public init(grapheme: Swift.String, phonemes: [Swift.String])
    public var description: Swift.String {
      get
    }
    public func insert(data: Speech.SFCustomLanguageModelData)
    public static func == (a: Speech.SFCustomLanguageModelData.CustomPronunciation, b: Speech.SFCustomLanguageModelData.CustomPronunciation) -> Swift.Bool
    public func encode(to encoder: any Swift.Encoder) throws
    public func hash(into hasher: inout Swift.Hasher)
    public var hashValue: Swift.Int {
      get
    }
    public init(from decoder: any Swift.Decoder) throws
  }
  @_functionBuilder public struct DataInsertableBuilder {
    public static func buildBlock(_ components: any Speech.DataInsertable...) -> any Speech.DataInsertable
    public static func buildEither(first: any Speech.DataInsertable) -> any Speech.DataInsertable
    public static func buildEither(second: any Speech.DataInsertable) -> any Speech.DataInsertable
    #if compiler(>=5.3) && $NonescapableTypes
    public static func buildOptional(_ component: (any Speech.DataInsertable)?) -> any Speech.DataInsertable
    #endif
    public static func buildArray(_ components: [any Speech.DataInsertable]) -> any Speech.DataInsertable
  }
  public class PhraseCountGenerator : Swift.Hashable, Swift.Codable, _Concurrency.AsyncSequence, Speech.DataInsertable {
    public typealias AsyncIterator = Speech.SFCustomLanguageModelData.PhraseCountGenerator.Iterator
    public typealias Element = Speech.SFCustomLanguageModelData.PhraseCount
    public func makeAsyncIterator() -> Speech.SFCustomLanguageModelData.PhraseCountGenerator.Iterator
    public init()
    @_hasMissingDesignatedInitializers public class Iterator : _Concurrency.AsyncIteratorProtocol {
      public typealias Element = Speech.SFCustomLanguageModelData.PhraseCount
      #if compiler(>=5.3) && $NonescapableTypes
      public func next() async throws -> Speech.SFCustomLanguageModelData.PhraseCount?
      #endif
      @available(iOS 18.0, tvOS 18.0, watchOS 11.0, visionOS 2.0, macOS 15.0, *)
      @_implements(_Concurrency.AsyncIteratorProtocol, Failure) public typealias __AsyncIteratorProtocol_Failure = any Swift.Error
      @objc deinit
    }
    public static func == (lhs: Speech.SFCustomLanguageModelData.PhraseCountGenerator, rhs: Speech.SFCustomLanguageModelData.PhraseCountGenerator) -> Swift.Bool
    public func hash(into hasher: inout Swift.Hasher)
    public func insert(data: Speech.SFCustomLanguageModelData)
    @available(iOS 18.0, tvOS 18.0, watchOS 11.0, visionOS 2.0, macOS 15.0, *)
    @_implements(_Concurrency.AsyncSequence, Failure) public typealias __AsyncSequence_Failure = any Swift.Error
    @objc deinit
    public func encode(to encoder: any Swift.Encoder) throws
    public var hashValue: Swift.Int {
      get
    }
    required public init(from decoder: any Swift.Decoder) throws
  }
  @_inheritsConvenienceInitializers public class TemplatePhraseCountGenerator : Speech.SFCustomLanguageModelData.PhraseCountGenerator {
    public struct Template : Swift.Hashable, Swift.Codable, Speech.TemplateInsertable {
      public let body: Swift.String
      public let count: Swift.Int
      public init(_ body: Swift.String, count: Swift.Int)
      public func insert(generator: Speech.SFCustomLanguageModelData.TemplatePhraseCountGenerator)
      public static func == (a: Speech.SFCustomLanguageModelData.TemplatePhraseCountGenerator.Template, b: Speech.SFCustomLanguageModelData.TemplatePhraseCountGenerator.Template) -> Swift.Bool
      public func encode(to encoder: any Swift.Encoder) throws
      public func hash(into hasher: inout Swift.Hasher)
      public var hashValue: Swift.Int {
        get
      }
      public init(from decoder: any Swift.Decoder) throws
    }
    @_hasMissingDesignatedInitializers public class Iterator : Speech.SFCustomLanguageModelData.PhraseCountGenerator.Iterator {
      public typealias Element = Speech.SFCustomLanguageModelData.PhraseCount
      public init(templates: [Speech.SFCustomLanguageModelData.TemplatePhraseCountGenerator.Template], templateClasses: [Swift.String : [Swift.String]])
      #if compiler(>=5.3) && $NonescapableTypes
      override public func next() async throws -> Speech.SFCustomLanguageModelData.PhraseCount?
      #endif
      @objc deinit
    }
    public func insert(template: Swift.String, count: Swift.Int)
    public func define(className: Swift.String, values: [Swift.String])
    override public func makeAsyncIterator() -> Speech.SFCustomLanguageModelData.PhraseCountGenerator.Iterator
    public static func == (lhs: Speech.SFCustomLanguageModelData.TemplatePhraseCountGenerator, rhs: Speech.SFCustomLanguageModelData.TemplatePhraseCountGenerator) -> Swift.Bool
    override public func hash(into hasher: inout Swift.Hasher)
    override public init()
    required public init(from decoder: any Swift.Decoder) throws
    @objc deinit
  }
  public struct CompoundTemplate : Speech.TemplateInsertable {
    public init(_ components: [any Speech.TemplateInsertable])
    public func insert(generator: Speech.SFCustomLanguageModelData.TemplatePhraseCountGenerator)
  }
  @_functionBuilder public struct TemplateInsertableBuilder {
    public static func buildBlock(_ components: any Speech.TemplateInsertable...) -> any Speech.TemplateInsertable
    public static func buildEither(first: any Speech.TemplateInsertable) -> any Speech.TemplateInsertable
    public static func buildEither(second: any Speech.TemplateInsertable) -> any Speech.TemplateInsertable
    #if compiler(>=5.3) && $NonescapableTypes
    public static func buildOptional(_ component: (any Speech.TemplateInsertable)?) -> any Speech.TemplateInsertable
    #endif
    public static func buildArray(_ components: [any Speech.TemplateInsertable]) -> any Speech.TemplateInsertable
  }
  public struct PhraseCountsFromTemplates : Speech.DataInsertable {
    public init(classes: [Swift.String : [Swift.String]], @Speech.SFCustomLanguageModelData.TemplateInsertableBuilder builder: () -> any Speech.TemplateInsertable)
    public func insert(data: Speech.SFCustomLanguageModelData)
  }
  final public let locale: Foundation.Locale
  final public let identifier: Swift.String
  final public let version: Swift.String
  public static func supportedPhonemes(locale: Foundation.Locale) -> [Swift.String]
  public init(locale: Foundation.Locale, identifier: Swift.String, version: Swift.String)
  convenience public init(locale: Foundation.Locale, identifier: Swift.String, version: Swift.String, @Speech.SFCustomLanguageModelData.DataInsertableBuilder builder: () -> any Speech.DataInsertable)
  public func insert(phraseCount: Speech.SFCustomLanguageModelData.PhraseCount)
  public func insert(phraseCountGenerator: Speech.SFCustomLanguageModelData.PhraseCountGenerator)
  public func insert(term: Speech.SFCustomLanguageModelData.CustomPronunciation)
  public func export(to path: Foundation.URL) async throws
  public static func == (lhs: Speech.SFCustomLanguageModelData, rhs: Speech.SFCustomLanguageModelData) -> Swift.Bool
  public func hash(into hasher: inout Swift.Hasher)
  @objc deinit
  public func encode(to encoder: any Swift.Encoder) throws
  public var hashValue: Swift.Int {
    get
  }
  required public init(from decoder: any Swift.Decoder) throws
}
@available(macOS 14.0, iOS 17.0, *)
extension Speech.SFAcousticFeature {
  public var acousticFeatureValuePerFrame: [Swift.Double] {
    get
  }
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
extension Speech.SFSpeechError.Code {
  public static var audioDisordered: Speech.SFSpeechError.Code {
    get
  }
  public static var unexpectedAudioFormat: Speech.SFSpeechError.Code {
    get
  }
  public static var noModel: Speech.SFSpeechError.Code {
    get
  }
  public static var assetLocaleNotAllocated: Speech.SFSpeechError.Code {
    get
  }
  public static var tooManyAssetLocalesAllocated: Speech.SFSpeechError.Code {
    get
  }
  public static var incompatibleAudioFormats: Speech.SFSpeechError.Code {
    get
  }
  public static var moduleOutputFailed: Speech.SFSpeechError.Code {
    get
  }
  public static var cannotAllocateUnsupportedLocale: Speech.SFSpeechError.Code {
    get
  }
  public static var insufficientResources: Speech.SFSpeechError.Code {
    get
  }
}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
extension Speech.AssetInventory.Status : Swift.Hashable {}
@available(macOS 26.0, iOS 26.0, visionOS 26.0, *)
@available(tvOS, unavailable)
@available(watchOS, unavailable)
extension Speech.SpeechDetector.SensitivityLevel : Swift.RawRepresentable {}
